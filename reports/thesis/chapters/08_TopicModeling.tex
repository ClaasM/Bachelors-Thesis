% !TEX root = ../main.tex
\chapter{Topic Modeling}
\label{ch:topicModeling}

Topic models are probabilistic models which are used to extract an underlying semantic structure from text artifacts, or documents.
They are based on a hierarchical bayesian analysis on the terms in a document,
on which a similarity measure for the its content can be defined and used to "connect" and cluster documents.
\par
Topic models are useful when dealing with large quantities of unstructured documents,
that need to be automatically structured, grouped or filtered for knowledge extraction,
for example to provide similar/related content at the end of blog entries,
or to automatically tag content with relevant keywords~\ref{blei2009topic}.
\par
In this chapter, methods for topic modeling will be explored,
and a suitable model for the use-case of this thesis will be determined.
While it is easy for humans to gain an \textit{intuitive} understanding what a document is about
and other subjective attributes, machines lack this intuition.
Therefore, the topic modeling methods presented in this chapter infer \textit{latent} topics,
meaning that they are unable to name or describe a topic, but merely confirm its existence.
However, since the topics inferred are dependent on term (in this case: word) frequencies,
the models can provide a human-understandable representation of a topic by listing its most frequent terms.

\section{Methods}
\label{sec:methods}

Similar to the previous chapter, multiple methods to perform topic modeling will be explained and demonstrated.
However, since no labeled dataset of a representative sample of statuses on Twitter and their respective topics is available,
only subjective assessments can be made to their performance.
Although the Sanders-dataset was collected by filtering a stream for specific keywords as described in~\ref{sec:theSandersDataset},
and these could be used as labels for a supervised approach such as supervised LDA~\ref{Blei2008},
the dataset is not representative of all activity on Twitter and not suitable for training such a model.
Therefore, only unsupervised approaches will be considered,
and subjective choices will be made as to what constitutes a "topic" in the context of this thesis,
with the parameters for the models being determined accordingly.

\subsection{TF-IDF}
\label{subsec:tfidf}

Both algorithms use TF-IDF (\textbf{T}erm \textbf{F}requency - \textbf{I}nverse \textbf{D}ocument \textbf{F}requency).
In the TF-IDF scheme, first, a dictionary of terms (words, usually) that the documents are made of is formed.
In this case, this dictionary will be created by taking all the terms (words) in the dataset,
and removing those that occur only once, to reduce its size.
Also, contrary to~\ref{ch:sentimentAnalysis}, in this chapter,
the tokenization function will also remove stopwords, as described in~\ref{sec:preprocessingAndtermization}.
Explain the difficulty to assess the right number of stopwords.
The implementation of the dictionary creation process can be seen in~\ref{code:create_corpus}.

\begin{figure}
    \caption{Implementation of corpus-creation.}
    \label{code:create_corpus}
    % @formatter:off
    \begin{minted}{python}
# The usual preprocessing and tokenization
tweets = [tokenize(preprocess(tweet) for tweet in tweets]

# Count term occurences
term_count_dictionary = defaultdict(int)
for tweet in tweets:
    for term in tweet:
        term_count_dictionary[term] += 1

# keep terms that occur more than once
terms = [[(term, term_count_dictionary[term]) for term in tweet if term_count_dictionary[term] > 1] for tweet in tweets]
    \end{minted}
    % @formatter:on
\end{figure}

\par
To compute the TF-IDF values for a specific document,
a term count vector is then formed containing how often each term in the dictionary occurs in that document,
and normalized over the length to result in the frequency.
This is usually sparsely implemented, since especially with short Twitter statuses,
most words in the corpus do not occur in the document.
Also, since some words like "the" might occur more often, thereby being less informative of the topic,
the frequency for each term in the document is then divided by the frequency of that term in the dictionary,
giving $term frequency / document frequency$, or $term frequency \* inverse document frequency$ (TF-IDF)~\cite{Blei2003}.
In this use case,the terms are terms, since
\par
The gensim library was used to create and save the TF-IDF matrix,
which contains the documents as rows and their respective TF-IDF values for each word in their columns~\cite{gensimDocs}.

\subsection{LDA}
\label{subsec:lda}

The basic idea behind LDA (\textbf{L}atent \textbf{D}irichlet \textbf{A}llocation) is to represent documents as a distribution of (latent) topics,
each of which is characterized as a distribution over words.
This thesis will provide an outside view of this method, while explaining important hyperparameters and their impact.
LDA is a bag-of-words model, which means there are no syntax rules.
\\
An LDA model takes the TF-IDF matrix created in~\ref{subsec:tfidf} as an input to determine the dirichlet distributions,
and two major hyperparameters, $\alpha$ and $\eta$,
both of which are parameters of the internal dirichlet-distributions.
\\
$\alpha$ controls the per-document topic dirichlet distribution,
where a higher value indicates that documents tend to be a mixture of many topics,
and lower values lead to few topics with higher probability.
\\
$\eta$ (also sometimes denoted $\beta$) controls the per-topic word dirichlet distribution, where, as with $\alpha$,
higher values lead to few words with high probabilities determining the topics,
and vice-versa.
\\
The output is a matrix where every row represents a topic,
the columns are the terms from the dictionary,
and every element represents the probability that the word belongs to the topic,
and every column therefore sums to 1.
When classifying a document, this matrix is used to allocate a distribution of topics for which the probability to have generated that document is maximized~\cite{Blei2003}.
\par
%First, try the default settings
For LDA topic modeling, the gensim library was used~\cite{gensimDocs}.
The dataset used was the streaming sample dataset described in~\ref{sec:streamingSampleDataset},
since the Sanders dataset is topically not representative of Twitter, as described in~\ref{sec:theSandersDataset}.
% TODO LDAvis was used for exploration + cite LDAvis docs + screenshot
The number of topics was set to 10.
$\eta$ was set to 0.01, which showed good results in previous Twitter topic modeling research~\cite{Hong2010}.
$\alpha$ is set to "auto", which means gensim learns it from the data provided.
% TODO Hoffman, Blei, Bach: Online Learning for Latent Dirichlet Allocation, for decay and offset (Kappa and Tau_0)
\par
A visualization as a network graph was devised to make a subjective assessment of the model.
In the graph, each light-blue node represents a topic.
For each topic, the 5 terms with the highest probabilities are connected with edges whose weight represents the probability.
It can be seen that some topics share terms - however, these shared terms are not very distinct terms.
The model even successfully modeled trending topics that evening.
The network graph can be seen in~\ref{fig:lda_network_graph}

\begin{figure}
    \centering
    \caption{Network graph of the LDA topic model created from the stream sample dataset}
    \label{fig:lda_network_graph}
    \includegraphics[width=\textwidth]{../figures/lda_network_graph.pdf}
\end{figure}


\subsection{NMF}
\label{subsec:nmf}

NMF (\textbf{N}onnegative \textbf{M}atrix \textbf{F}actorization) is another active area of research in topic modeling.
It overcomes some of the shortcomings of LDA in terms of consistency across runs and empirical convergence,
since it is deterministic~\cite{Choo2013}.
It is also easier to adjust the model and incorporate feedback on the classification results in a semi-supervised approach,
making it a potentially more suitable solution for recommendation systems as described at the beginning of this chapter.
The output of the model is the same as the output of an LDA model: a term-wise representation of topics, even though the columns
do not necessarily sum up to 1.

% TODO maybe show the optimization function here

The implementation of NMF in the scikit-learn library was used~\cite{scikitDocs}.
The number of components (the number of topics in this case) was set to 10 to ensure comparability with the LDA results.
Although several parameters were explored, the best results were accomplished with the default parameters.
The resulting network graph can be seen in~\ref{fig:nmf_network_graph}.

\begin{figure}
    \centering
    \caption{Network graph of the NMF topic model created from the stream sample dataset}
    \label{fig:nmf_network_graph}
    \includegraphics[width=\textwidth]{../figures/nmf_network_graph.pdf}
\end{figure}

\section{Results}
\label{sec:results}

Overall, the results that could be achieved with both methods were similar in terms of subjective descriptiveness of the top terms and amount of overlap.
There were even some equal topics, like topic number 6 in the LDA model and topic number 1 in the NMF model.
The advantages of NMF also do not apply to this usecase, therefore a decision was made in favor or LDA,
since it is the more mature method with more available research and implementations. % TODO like LDAvis