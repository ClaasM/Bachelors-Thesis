% !TEX root = ../main.tex
\chapter{Detailed Descriptions}
%\section{Detailed Validation Results}
\label{chapter:DetailedDescriptions}\label{sentimentanalysis}
%\inputminted{c++}{../../src/wos_native.cuh}

% TODO redo all of these analyses, filter out non-english tweets for all these

Explaining all the methods used and what classes they classify into, how they work "under the hood"

\section{Methods}
\label{sec:methods}

\subsection{Na\"{i}ve Bayes}
\label{subsec:naivebayes}

Using the implementation from the NLTK

\subsection{NLTK Sentiment Analyzer}
\label{subsec:nltksentimentanalyzer}

\subsection{VADER}
\label{subsec:vader}

% TODO Interestingly, Vader performs better on the dataset provided by the nltk (which is collected using...) than the sanders-dataset
Also using the implementation from the NLTK

\subsection{TextBlob}
\label{subsec:textblob}

Failed to exceed 50\% accuracy on tweets, preprocessed or not, because of reasons

\subsection{GCloud}
\label{subsec:gcloud}

Languages were filtered out.
Include some performance data from the API, like latency and stuff

\section{Comparison}
\label{sec:comparison}

Quantitative and qualitative comparison of different kinds with supporting figures

\subsection{Discrepancies in Performance}
\label{subsec:discrepanciesinperformance}

%TODO Discrepancies in performance between dictionaries are indicative of the sentiment model not generalizing well over all of twitter.
%TODO Also, Twitter statuses pose a challenge because they are short.
Explaining discrepancies in performance I encountered during evaluation of the different methods.

\subsection{Comparing to results without preprocessing}
\label{subsec:comparingToResultsWithoutPreprocessing}

Once again proving the point of preprocessing by showing that the results would have been worse without it (in a table or something)
Or this maybe driving it past home?

4 pages planned for this chapter.
\pagebreak[4]
